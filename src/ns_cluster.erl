%% @author Couchbase <info@couchbase.com>
%% @copyright 2009-Present Couchbase, Inc.
%%
%% Use of this software is governed by the Business Source License included
%% in the file licenses/BSL-Couchbase.txt.  As of the Change Date specified
%% in that file, in accordance with the Business Source License, use of this
%% software will be governed by the Apache License, Version 2.0, included in
%% the file licenses/APL2.txt.
%%
-module(ns_cluster).

-behaviour(gen_server).

-include("ns_common.hrl").
-include("cut.hrl").

-ifdef(TEST).
-include_lib("eunit/include/eunit.hrl").
-endif.

-define(UNUSED_NODE_JOIN_REQUEST, 2).
-define(NODE_JOINED, 3).
-define(NODE_EJECTED, 4).
-define(NODE_JOIN_FAILED, 5).

-define(ADD_NODE_TIMEOUT,       ?get_timeout(add_node, 240000)).
-define(ENGAGE_TIMEOUT,         ?get_timeout(engage, 30000)).
-define(COMPLETE_TIMEOUT,       ?get_timeout(complete, 240000)).
-define(PREP_CHRONICLE_TIMEOUT, ?get_timeout(prep_chronicle, 240000)).
-define(JOIN_CHRONICLE_TIMEOUT, ?get_timeout(join_chronicle, 240000)).
-define(CHANGE_ADDRESS_TIMEOUT, ?get_timeout(change_address, 30000)).

-define(cluster_log(Code, Fmt, Args),
        ale:xlog(?USER_LOGGER, ns_log_sink:get_loglevel(?MODULE, Code),
                 {?MODULE, Code}, Fmt, Args)).

-define(cluster_debug(Fmt, Args), ale:debug(?CLUSTER_LOGGER, Fmt, Args)).
-define(cluster_info(Fmt, Args), ale:info(?CLUSTER_LOGGER, Fmt, Args)).
-define(cluster_warning(Fmt, Args), ale:warn(?CLUSTER_LOGGER, Fmt, Args)).
-define(cluster_error(Fmt, Args), ale:error(?CLUSTER_LOGGER, Fmt, Args)).

%% gen_server callbacks
-export([code_change/3, handle_call/3, handle_cast/2, handle_info/2, init/1,
         terminate/2]).

%% API
-export([leave/0,
         leave/1,
         leave_async/0,
         shun/1,
         start_link/0]).

-export([add_node_to_group/6,
         engage_cluster/1, complete_join/1,
         prep_chronicle/2, join_chronicle/2,
         check_host_port_connectivity/2, change_address/1,
         enforce_topology_limitation/1,
         rename_marker_path/0,
         sanitize_node_info/1,
         verify_otp_connectivity/2]).

%% debugging & diagnostic export
-export([do_change_address/2]).

-export([counters/0,
         counter/3,
         counter_inc/1,
         counter_inc/2]).

-record(state, {}).

%%
%% API
%%

start_link() ->
    gen_server:start_link({local, ?MODULE}, ?MODULE, [], []).

add_node_to_group(Scheme, RemoteAddr, RestPort, Auth, GroupUUID, Services) ->
    RV = gen_server:call(?MODULE, {add_node_to_group, Scheme, RemoteAddr, RestPort, Auth, GroupUUID, Services},
                         ?ADD_NODE_TIMEOUT),
    case RV of
        {error, _What, Message} ->
            ?cluster_log(?NODE_JOIN_FAILED,
                         "Failed to add node ~s:~w to cluster. ~s",
                         [RemoteAddr, RestPort, Message]);
        _ -> ok
    end,
    RV.

engage_cluster(NodeKVList) ->
    MyNode = node(),
    RawOtpNode = proplists:get_value(<<"otpNode">>, NodeKVList, <<"undefined">>),
    OtpNode = binary_to_atom(RawOtpNode, latin1),
    Joinable = ns_cluster_membership:system_joinable(),
    if
        OtpNode =:= MyNode ->
            {error, self_join, <<"Joining node to itself is not allowed.">>};
        not Joinable ->
            {error, system_not_joinable,
             <<"Node is already part of cluster.">>};
        true ->
            engage_cluster_not_to_self(NodeKVList)
    end.

engage_cluster_not_to_self(NodeKVList) ->
    case cluster_compat_mode:tls_supported() of
        true ->
            engage_cluster_apply_certs(NodeKVList);
        false ->
            engage_cluster_apply_net_config(NodeKVList)
    end.

engage_cluster_apply_certs(NodeKVList) ->
    case proplists:get_value(<<"clusterCA">>, NodeKVList) of
        undefined ->
            NeedEncryption =
                proplists:get_value(<<"nodeEncryption">>, NodeKVList, false),
            case {proplists:get_value(<<"autogeneratedCA">>, NodeKVList),
                  NeedEncryption} of
                {Cert, true} when Cert =/= undefined ->
                    NodeCert = proplists:get_value(<<"autogeneratedCert">>,
                                                   NodeKVList),
                    NodeKey =
                        %% 7.0+ nodes wrap it in {otpCookie, } to make sure it
                        %% is sanitized by pre 7.0 nodes
                        case proplists:get_value(<<"autogeneratedKey">>,
                                                 NodeKVList) of
                            {struct, [{<<"otpCookie">>, K}]} -> K;
                            K -> K
                        end,
                    HostBin = proplists:get_value(
                                <<"requestedTargetNodeHostname">>, NodeKVList),
                    Host = binary_to_list(HostBin),
                    ns_ssl_services_setup:set_certs(Host, Cert, NodeCert,
                                                    NodeKey),
                    ns_ssl_services_setup:sync(),
                    ?log_info("Generated certificate was loaded on the node "
                              "before joining. Cert: ~p", [Cert]);
                _ ->
                    ok
            end,
            engage_cluster_apply_net_config(NodeKVList);
        ClusterCA ->
            case apply_certs(ClusterCA) of
                ok -> engage_cluster_apply_net_config(NodeKVList);
                {error, _, _} = Error -> Error
            end
    end.

apply_certs(ClusterCA) ->
    case ns_server_cert:add_CAs(uploaded, ClusterCA) of
        {ok, _} ->
            %% We skip loading if certs in inbox are already loaded because of
            %% two reasons:
            %%  - We don't have passphrase settings for encrypted key, so if we
            %%    try to load them one more time, we will fail (in case if pkey
            %%    is encrypted);
            %%  - No point in reloading the same certs twice anyway
            %% Note 1:
            %% The check is implemented outside of load_node_certs_from_inbox
            %% because it actually does make sense calling
            %% load_node_certs_from_inbox with unchanged cert files,
            %% when only passphrase settings change.
            %% Note 2:
            %% This loading of certs from engage_cluster is only useful
            %% when node is loaded via http (not https). In case of https
            %% certificates must already be loaded by this moment, we won't be
            %% able to establish a TLS connection for engage_cluster otherwise.
            case ns_server_cert:are_certs_loaded() orelse
                 ns_server_cert:load_node_certs_from_inbox([]) of
                true ->
                    ?log_info("Certificates are already loaded, skipping "
                              "apply_certs stage"),
                    ok;
                {ok, Props} ->
                    ns_ssl_services_setup:sync(),
                    ?log_info("Custom certificate was loaded on the node "
                              "before joining. Props: ~p", [Props]),
                    ok;
                {error, Error} ->
                    Message =
                        iolist_to_binary(
                          ["Error applying node certificate. ",
                           ns_error_messages:reload_node_certificate_error(
                             Error)]),
                    {error, apply_cert, Message}
            end;
        {error, Error} ->
            Msg = io_lib:format("Error applying root CA: ~p", [Error]),
            {error, apply_cert, iolist_to_binary(Msg)}
    end.

engage_cluster_apply_net_config(NodeKVList) ->
    case apply_net_config(NodeKVList) of
        ok -> call_engage_cluster(NodeKVList);
        {error, Msg} ->
            {error, apply_net_config, Msg}
    end.

apply_net_config(NodeKVList) ->
    case ensure_dist_ports_match(NodeKVList) of
        ok ->
            case extract_remote_cluster_net_settings(NodeKVList) of
                {ok, AFamily, AFamilyOnly, NEncryption, Protos} ->
                    ?log_info("Applying net config. AFamily: ~p, "
                              "AFamilyOnly: ~p, "
                              "NEncryption: ~p, "
                              "DistProtos: ~p",
                              [AFamily, AFamilyOnly, NEncryption, Protos]),
                    Props = [{externalListeners, Protos},
                             {afamily, AFamily},
                             {afamilyOnly, AFamilyOnly},
                             {nodeEncryption, NEncryption}],
                    case netconfig_updater:apply_config(Props) of
                        ok ->
                            ns_config:sync_announcements(),
                            menelaus_event:sync(ns_config_events),
                            cluster_compat_mode:is_enterprise() andalso
                                ns_ssl_services_setup:sync(),
                            ok;
                        {error, Msg} -> {error, Msg}
                    end;
                {error, Msg} -> {error, Msg}
            end;
        {error, Msg} -> {error, Msg}
    end.

extract_remote_cluster_net_settings(NodeKVList) ->
    Listeners = case proplists:get_value(<<"externalListeners">>, NodeKVList) of
                    undefined -> undefined;
                    Pairs ->
                       lists:map(
                         fun ({struct, P}) ->
                                 [{<<"afamily">>, AF},
                                  {<<"nodeEncryption">>, NE}] = lists:usort(P),
                                 {binary_to_atom(AF, latin1), NE}
                         end, Pairs)
                end,
    NEncryption = proplists:get_value(<<"nodeEncryption">>, NodeKVList,
                                      false),
    AFamilyOnly = proplists:get_value(<<"addressFamilyOnly">>, NodeKVList,
                                      false),
    case proplists:get_value(<<"addressFamily">>, NodeKVList) of
        undefined ->
            case pre_65_remote_node_address_family(NodeKVList) of
                {ok, AF} -> {ok, AF, AFamilyOnly, NEncryption, Listeners};
                {error, Msg} -> {error, Msg}
            end;
        AFamilyBin ->
            AFamily = binary_to_atom(AFamilyBin, latin1),
            {ok, AFamily, AFamilyOnly, NEncryption, Listeners}
    end.

%% Pre 6.5 nodes do not include address family info in engageCluster.
%% In order to figure it out we can check the type of port that is used for
%% distribution. If that port is ipv6 the remote node is ipv6 node.
pre_65_remote_node_address_family(NodeKVList) ->
    RemoteNodeBin = proplists:get_value(<<"otpNode">>, NodeKVList, <<>>),
    RemoteNode = binary_to_atom(RemoteNodeBin, latin1),
    {Name, Host} = misc:node_name_host(RemoteNode),
    case {misc:is_raw_ip(Host), misc:is_raw_ipv6(Host)} of
        {true, false} -> {ok, inet};
        {true, true} -> {ok, inet6};
        {false, _} ->
            case pre_65_call_port_please(Name, Host) of
                {ok, Port} ->
                    case check_host_port_connectivity(Host, Port, inet6) of
                        {ok, _} -> {ok, inet6};
                        {error, _} -> {ok, inet}
                    end;
                {error, _} = Error ->
                    Msg = ns_error_messages:verify_otp_connectivity_port_error(
                            RemoteNode, Host, Error),
                    {error, Msg}
            end
    end.

pre_65_call_port_please(Name, Host) ->
    case resolve(Host) of
        {ok, IPList} ->
            lists:foldl(
              fun ({IP, _AFamily}, {error, _}) ->
                      case erl_epmd:port_please(Name, IP, 5000) of
                          {port, P, _} -> {ok, P};
                          _ -> {error, noport}
                      end;
                  (_, Res) -> Res
              end, {error, undefined}, IPList);
        {error, Errors} -> {error, hd(Errors)}
    end.

ensure_dist_ports_match(NodeKVList) ->
    {struct, Ports} = proplists:get_value(<<"ports">>, NodeKVList,
                                          {struct, []}),
    RemoteNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),
    RemoteShortNode = misc:node_name_short(RemoteNode),
    LocalTCPPort = cb_epmd:port_for_node(inet_tcp_dist, RemoteShortNode),
    RemoteTCPPort = proplists:get_value(<<"distTCP">>, Ports, LocalTCPPort),
    LocalTLSPort = cb_epmd:port_for_node(inet_tls_dist, RemoteShortNode),
    RemoteTLSPort = proplists:get_value(<<"distTLS">>, Ports, LocalTLSPort),
    case {RemoteTCPPort, RemoteTLSPort} of
        {LocalTCPPort, LocalTLSPort} -> ok;
        {LocalTCPPort, _} ->
            Msg = io_lib:format("Distribution TLS ports doesn't match. Cluster "
                                "uses ~p while new node will use ~p port",
                                [RemoteTLSPort, LocalTLSPort]),
            {error, iolist_to_binary(Msg)};
        {_, _} ->
            Msg = io_lib:format("Distribution ports doesn't match. Cluster "
                                "uses ~p while new node will use ~p port",
                                [RemoteTCPPort, LocalTCPPort]),
            {error, iolist_to_binary(Msg)}
    end.

call_engage_cluster(NodeKVList) ->
    gen_server:call(?MODULE, {engage_cluster, NodeKVList}, ?ENGAGE_TIMEOUT).

complete_join(NodeKVList) ->
    gen_server:call(?MODULE, {complete_join, NodeKVList}, ?COMPLETE_TIMEOUT).

multi_call(Nodes, Call, Timeout) ->
    {_Replies, BadNodes} =
        misc:multi_call(Nodes, ?MODULE, Call, Timeout, _ =:= ok),
    case BadNodes of
        [] ->
            ok;
        _ ->
            {bad_nodes, BadNodes}
    end.

prep_chronicle(Nodes, Info) ->
    multi_call(Nodes, {prep_chronicle, Info}, ?PREP_CHRONICLE_TIMEOUT).

join_chronicle(Nodes, Info) ->
    multi_call(Nodes, {join_chronicle, Info}, ?JOIN_CHRONICLE_TIMEOUT).

-spec change_address(string()) -> ok
                                  | {cannot_resolve, {inet:posix(), inet|inet6}}
                                  | {cannot_listen, inet:posix()}
                                  | not_self_started
                                  | {address_save_failed, any()}
                                  | {address_not_allowed, string()}
                                  | already_part_of_cluster
                                  | not_renamed.
change_address(Address) ->
    case misc:is_good_address(Address) of
        ok ->
            gen_server:call(?MODULE, {change_address, Address}, ?CHANGE_ADDRESS_TIMEOUT);
        Error ->
            Error
    end.

%% @doc Returns proplist of cluster-wide counters.
counters() -> counters(direct).

counters(Snapshot) ->
    Counters = chronicle_compat:get(Snapshot, counters, #{default => []}),
    lists:map(
      fun ({Name, {_Timestamp, Value}}) -> {Name, Value};
          %% backward compat for counters that we kept in ns_config:
          (Pair) -> Pair
      end, Counters).

counter(Snapshot, CounterName, Default) ->
    proplists:get_value(CounterName, counters(Snapshot), Default).

%% @doc Increment a cluster-wide counter.
counter_inc(CounterName) ->
    % We expect counters to be slow moving (num rebalances, num failovers,
    % etc), and favor efficient reads over efficient writes.
    case chronicle_compat:backend() of
        chronicle ->
            chronicle_kv:transaction(
              kv,
              [counters],
              fun (Snapshot) ->
                  {Counters, _} = maps:get(counters, Snapshot, {[], undefined}),
                  {_, OldValue} = proplists:get_value(CounterName, Counters,
                                                      {undefined, 0}),
                  TS = os:system_time(second),
                  NewCounters = [{CounterName, {TS, OldValue + 1}} |
                                 proplists:delete(CounterName, Counters)],
                  {commit, [{set, counters, NewCounters}]}
              end);
        ns_config ->
            %% Here we assume that counters can't be modified between upgrade
            %% start and change of compat mode (because both things are done by
            %% the orchestrator), so if we get to this point it means that
            %% upgrade hasn't started yet and it's safe to store data in
            %% ns_config
            PList = counters(),
            Value = proplists:get_value(CounterName, PList, 0) + 1,
            ok = ns_config:set(counters,
                               [{CounterName, Value} |
                                proplists:delete(CounterName, PList)])
    end.

counter_inc(Type, Name)
  when is_atom(type), is_atom(Name) ->
    counter_inc(list_to_atom(atom_to_list(Type) ++ "_" ++ atom_to_list(Name))).

%%
%% gen_server handlers
%%

code_change(_OldVsn, State, _Extra) ->
    {ok, State}.

sanitize_node_info(NodeKVList) ->
    misc:rewrite_tuples(
      fun ({<<"otpCookie">>, Cookie}) ->
              {stop, {<<"otpCookie">>, ns_cookie_manager:sanitize_cookie(Cookie)}};
          ({otpCookie, Cookie}) ->
              {stop, {otpCookie, ns_cookie_manager:sanitize_cookie(Cookie)}};
          ({<<"autogeneratedKey">>, _}) ->
              {stop, {<<"autogeneratedKey">>, <<"********">>}};
          (_Other) ->
              continue
      end, NodeKVList).

handle_call({add_node_to_group, Scheme, RemoteAddr, RestPort, Auth, GroupUUID,
             Services}, _From, State) ->
    ?cluster_debug("handling add_node(~p, ~p, ~p, ~p, ..)",
                   [Scheme, RemoteAddr, RestPort, GroupUUID]),
    RV = do_add_node(Scheme, RemoteAddr, RestPort, Auth, GroupUUID, Services),
    ?cluster_debug("add_node(~p, ~p, ~p, ~p, ..) -> ~p",
                   [Scheme, RemoteAddr, RestPort, GroupUUID, RV]),
    {reply, RV, State};

handle_call({engage_cluster, NodeKVList}, _From, State) ->
    ?cluster_debug("handling engage_cluster(~p)", [sanitize_node_info(NodeKVList)]),
    RV = do_engage_cluster(NodeKVList),
    ?cluster_debug("engage_cluster(..) -> ~p", [RV]),
    {reply, RV, State};

handle_call({complete_join, NodeKVList}, _From, State) ->
    ?cluster_debug("handling complete_join(~p)", [sanitize_node_info(NodeKVList)]),
    RV = do_complete_join(NodeKVList),
    ?cluster_debug("complete_join(~p) -> ~p", [sanitize_node_info(NodeKVList), RV]),

    case RV of
        %% we failed to start ns_server back; we don't want to stay in this
        %% state, so perform_actual_join has created a marker file indicating
        %% that somebody needs to attempt to start ns_server back; usually
        %% this somebody is ns_cluster:init; so to let it do its job we need
        %% to restart ns_cluster; note that we still reply to the caller
        {error, start_cluster_failed, _} ->
            {stop, start_cluster_failed, RV, State};
        _ ->
            {reply, RV, State}
    end;

handle_call({change_address, Address}, _From, State) ->
    ?cluster_info("Changing address to ~p due to client request", [Address]),
    RV = case ns_cluster_membership:system_joinable() of
             true ->
                 %% we're the only node in the cluster; allowing rename
                 do_change_address(Address, true);
             false ->
                 already_part_of_cluster
         end,
    {reply, RV, State};

handle_call({prep_chronicle, Info}, _From, State) ->
    chronicle_compat_events:hush_chronicle(),
    ?log_debug("Preparing chronicle to join cluster"),
    ok = chronicle_local:prepare_join(Info),
    ?log_debug("Successfully prepared chronicle"),

    {reply, ok, State};

handle_call({join_chronicle, Info}, _From, State) ->
    ?log_debug("Received request to join cluster"),
    ok = chronicle_local:join_cluster(Info),
    chronicle_compat_events:resume_chronicle(),
    ?log_debug("Successfully joined cluster"),

    {reply, ok, State}.

handle_cast(leave, State) ->
    ?cluster_log(0001, "Node ~p is leaving cluster.", [node()]),

    misc:create_marker(leave_marker_path()),

    %% empty users storage
    users_storage_sup:stop_replicator(),
    menelaus_users:empty_storage(),

    %% stop nearly everything
    ok = ns_server_cluster_sup:stop_ns_server(),

    perform_leave(),

    ?cluster_debug("Leaving cluster", []),

    misc:create_marker(start_marker_path()),
    misc:remove_marker(leave_marker_path()),

    {ok, _} = ns_server_cluster_sup:start_ns_server(),
    ns_ports_setup:restart_memcached(),

    misc:remove_marker(start_marker_path()),

    {noreply, State};
handle_cast(repair_join, State) ->
    ?log_debug("Repair after unsuccessful join."),
    perform_leave(),

    misc:remove_marker(join_marker_path()),
    {noreply, State};

handle_cast(retry_start_ns_server, State) ->
    case ns_server_cluster_sup:start_ns_server() of
        {ok, _} ->
            ok;
        {error, running} ->
            ?log_warning("ns_server is already running. Ignoring."),
            ok;
        Other ->
            exit({failed_to_start_ns_server, Other})
    end,

    ns_ports_setup:restart_memcached(),

    misc:remove_marker(start_marker_path()),

    {noreply, State}.

handle_info(check_chronicle_state, State) ->
    ChronicleState = chronicle:get_system_state(),
    ?log_info("Chronicle state is: ~p", [ChronicleState]),
    case ChronicleState of
        removed ->
            true = chronicle_compat:enabled(),
            false = misc:marker_exists(leave_marker_path()),
            handle_cast(leave, State);
        _ ->
            ok
    end,
    {noreply, State};

handle_info(Msg, State) ->
    ?cluster_debug("Unexpected message ~p, State = ~p", [Msg, State]),
    {noreply, State}.


init([]) ->
    Self = self(),

    Subscribe =
        fun () ->
                ns_pubsub:subscribe_link(
                  chronicle_external_events,
                  fun ({important_change, system_state}) ->
                          Self ! check_chronicle_state;
                      (_) ->
                          ok
                  end)
        end,

    case misc:marker_exists(leave_marker_path()) of
        true ->
            ?log_info("Found marker of in-flight cluster leave. "
                      "Looks like previous leave procedure crashed. "
                      "Going to complete leave cluster procedure."),
            %% we have to do it async because otherwise our parent
            %% supervisor is waiting us to complete init and our call
            %% to terminate ns_server_sup is going to cause deadlock
            gen_server:cast(Self, leave),
            Subscribe();
        false ->
            case misc:marker_exists(start_marker_path()) of
                true ->
                    case misc:marker_exists(join_marker_path()) of
                        true ->
                            ?log_info("Found marker ~p. "
                                      "Looks like we failed in process of "
                                      "joining cluster. Will restore config "
                                      "to clean state. ", [join_marker_path()]),
                            gen_server:cast(Self, repair_join);
                        false ->
                            ok
                    end,
                    ?log_info("Found marker ~p. "
                              "Looks like we failed to restart ns_server "
                              "after leaving or joining a cluster. "
                              "Will try again.", [start_marker_path()]),
                    gen_server:cast(Self, retry_start_ns_server);
                false ->
                    ok
            end,
            Subscribe(),
            Self ! check_chronicle_state
    end,
    {ok, #state{}}.


terminate(_Reason, _State) ->
    ok.


%%
%% Internal functions
%%


%%
%% API
%%

leave() ->
    RemoteNode = ns_node_disco:another_live_node(),
    ?cluster_log(?NODE_EJECTED, "Node ~s asked to leave the cluster", [node()]),
    %% MB-3160: sync any pending config before we leave, to make sure,
    %% say, deactivation of membership isn't lost
    ok = ns_config_rep:ensure_config_seen_by_nodes([RemoteNode]),
    ?cluster_debug("ns_cluster: leaving the cluster from ~p.",
                   [RemoteNode]),
    %% Tell the remote server to tell everyone to shun me.
    rpc:cast(RemoteNode, ?MODULE, shun, [node()]),
    case chronicle_compat:enabled() of
        false ->
            %% Then drop ourselves into a leaving state.
            leave_async();
        true ->
            ok
    end.

%% Cause another node to leave the cluster if it's up
leave(Node) ->
    ?cluster_debug("Asking node ~p to leave the cluster", [Node]),

    case Node =:= node() of
        true ->
            leave();
        false ->
            %% Will never fail, but may not reach the destination
            case chronicle_compat:enabled() of
                false ->
                    gen_server:cast({?MODULE, Node}, leave);
                true ->
                    ok
            end,
            shun(Node)
    end.

%% @doc Just trigger the leave code; don't get another node to shun us.
leave_async() ->
    gen_server:cast(?MODULE, leave).

%% Note that shun does *not* cause the other node to reset its config!
shun(RemoteNode) ->
    case RemoteNode =:= node() of
        false ->
            try
                ?cluster_debug("Shunning ~p", [RemoteNode]),
                case chronicle_master:remove_peer(RemoteNode) of
                    ok ->
                        ns_config_rep:ensure_config_pushed();
                    delegated_operation ->
                        ok
                end
            catch T:E:Stack ->
                    ?log_error("Shun failed with ~p", [{T,E,Stack}]),
                    exit(shun_failed)
            end;
        true ->
            ?cluster_debug("Asked to shun myself. Leaving cluster.", []),
            leave()
    end.

check_host_port_connectivity(Host, Port, AFamily) ->
    try gen_tcp:connect(Host, Port, [AFamily, binary, {packet, 0},
                                     {active, false}], 5000) of
        {ok, Socket} ->
            try
                {ok, {IpAddr, _}} = inet:sockname(Socket),
                ?log_debug("Successfully checked TCP connectivity to "
                           "~p:~p", [Host, Port]),
                {ok, inet:ntoa(IpAddr)}
            after
                inet:close(Socket)
            end;
        {error, nxdomain} ->
            {error, {AFamily, nxdomain}};
        {error, Reason} ->
            {error, Reason}
    catch
        _:R -> {error, R}
    end.

resolve(Host) ->
    Res = [{inet:getaddr(Host, AF), AF} || AF <- [inet, inet6]],
    IPs = [{IP, AF} || {{ok, IP}, AF} <- Res],
    Errors = [R || {{error, R}, _} <- Res],
    case IPs of
        [] -> {error, Errors};
        _ -> {ok, IPs}
    end.

check_host_port_connectivity(Host, Port) ->
    case resolve(Host) of
        {ok, IPList} ->
            lists:foldl(
              fun ({IP, AFamily}, {error, _}) ->
                      case check_host_port_connectivity(IP, Port, AFamily) of
                          {ok, MyIP} -> {ok, MyIP, AFamily};
                          {error, Error} -> {error, Error}
                      end;
                  (_, Res) ->
                      Res
              end, {error, undefined}, IPList);
        {error, Errors} ->
            {error, hd(Errors)}
    end.

-spec do_change_address(string(), boolean()) -> ok | not_renamed |
                                                {address_save_failed, _} | not_self_started.
do_change_address(NewAddr, UserSupplied) ->
    NewAddr1 =
        case UserSupplied of
            false ->
                misc:get_env_default(rename_ip, NewAddr);
            true ->
                NewAddr
        end,

    ?cluster_info("Change of address to ~p is requested.", [NewAddr1]),
    case maybe_rename(NewAddr1, UserSupplied) of
        not_renamed ->
            not_renamed;
        renamed ->
            ns_server_sup:node_name_changed(),
            ?cluster_info("Renamed node. New name is ~p.", [node()]),
            ok;
        Other ->
            Other
    end.

maybe_rename(NewAddr, UserSupplied) ->
    OldName = node(),
    OnRename =
        fun() ->
                %% prevent node disco events while we're in the middle
                %% of renaming
                ns_node_disco:register_node_renaming_txn(self()),

                %% prevent breaking remote monitors while we're in the middle
                %% of renaming
                remote_monitors:register_node_renaming_txn(self())
        end,

    case dist_manager:adjust_my_address(NewAddr, UserSupplied, OnRename) of
        nothing ->
            ?cluster_debug("Not renaming node.", []),
            not_renamed;
        not_self_started ->
            ?cluster_debug("Didn't rename the node because net_kernel "
                           "is not self started", []),
            not_self_started;
        {address_save_failed, _} = Error ->
            Error;
        net_restarted ->
            ?cluster_debug("Renamed node from ~p to ~p.", [OldName, node()]),
            renamed
    end.

check_add_possible(Body) ->
    case ns_config_auth:is_system_provisioned() of
        false -> {error, system_not_provisioned,
                  <<"Adding nodes to not provisioned nodes is not allowed.">>};
        true ->
            case rebalance:running() of
                true ->
                    Msg = <<"Node addition is disallowed while rebalance "
                            "is in progress">>,
                    {error, rebalance_running, Msg};
                false ->
                    Body()
            end
    end.

do_add_node(Scheme, RemoteAddr, RestPort, Auth, GroupUUID, Services) ->
    check_add_possible(
      fun () ->
              do_add_node_allowed(Scheme, RemoteAddr, RestPort, Auth,
                                  GroupUUID, Services)
      end).

should_change_address() ->
    %% adjust our name if we're alone
    ns_node_disco:nodes_wanted() =:= [node()] andalso
        not dist_manager:using_user_supplied_address().

do_add_node_allowed(Scheme, RemoteAddr, RestPort, Auth, GroupUUID, Services) ->
    case check_host_port_connectivity(RemoteAddr, RestPort,
                                      misc:get_net_family()) of
        {ok, MyIP} ->
            R = case should_change_address() of
                    true ->
                        case do_change_address(MyIP, false) of
                            {address_save_failed, _} = E ->
                                E;
                            not_self_started ->
                                ?cluster_debug("Haven't changed address because of not_self_started condition", []),
                                ok;
                            not_renamed ->
                                ok;
                            ok ->
                                ok
                        end;
                    _ ->
                        ok
                end,
            case R of
                ok ->
                    do_add_node_with_connectivity(Scheme, RemoteAddr, RestPort,
                                                  Auth, GroupUUID, Services);
                {address_save_failed, Error} ->
                    Msg = io_lib:format(
                            "Could not save address after rename: ~p", [Error]),
                    {error, rename_failed, iolist_to_binary(Msg)}
            end;
        {error, Reason} ->
            M = case ns_error_messages:connection_error_message(
                       Reason, RemoteAddr, RestPort) of
                    undefined -> io:format("~p", [Reason]);
                    Msg -> Msg
                end,
            URL = menelaus_rest:rest_url(RemoteAddr, RestPort, "", Scheme),
            ReasonStr = io_lib:format("Failed to connect to ~s. ~s", [URL, M]),
            {error, host_connectivity, iolist_to_binary(ReasonStr)}
    end.

post_json(Target, Auth, Options, Stuff) ->
    ?cluster_debug("Posting the following to ~p:~n~p",
                   [Target, {sanitize_node_info(Stuff)}]),
    Post = list_to_tuple(Target ++
                             ["application/json", mochijson2:encode(Stuff)]),
    RV = menelaus_rest:json_request_hilevel(post, Post, Auth, Options),
    ?cluster_debug("Reply from ~p:~n~p", [Target, sanitize_node_info(RV)]),

    case RV of
        {client_error, [Message]} when is_binary(Message) ->
            {error, Message};
        {client_error, _} ->
            {error, invalid_json};
        {error, _, X, _} ->
            {error, X};
        Other ->
            Other
    end.

engage_cluster_bad_json_error(Exc) ->
    {error, engage_cluster_bad_json,
     ns_error_messages:engage_cluster_json_error(Exc)}.

do_add_node_with_connectivity(Scheme, RemoteAddr, RestPort, Auth, GroupUUID,
                              Services) ->
    {struct, NodeInfo} = menelaus_web_node:build_full_node_info(node()),
    IsPreviewCluster = cluster_compat_mode:is_developer_preview(),
    Props = [{<<"requestedTargetNodeHostname">>, list_to_binary(RemoteAddr)},
             {<<"requestedServices">>, Services},
             {<<"isDeveloperPreview">>, IsPreviewCluster}]
        ++ NodeInfo,

    Encryption = proplists:get_bool(nodeEncryption, Props),

    Config = ns_config:get(),

    Props1 =
        case ns_server_cert:this_node_uses_self_generated_certs(Config) of
            false ->
                CA = ns_server_cert:this_node_ca(Config),
                [{<<"clusterCA">>, CA}];
            true when Encryption ->
                {CA, NewNodeCert, NewNodeKey} =
                    case ns_server_cert:generate_node_certs(RemoteAddr) of
                        no_private_key ->
                            _ = ns_server_cert:generate_and_set_cert_and_pkey(),
                            ns_server_cert:generate_node_certs(RemoteAddr);
                        Certs -> Certs
                    end,
                %% Sending PKey only in case if encryption is enabled (so
                %% remote node really needs it) and only if cluster uses
                %% autogenerated certs (otherwise user is supposed to provision
                %% new node with certs)
                [{<<"autogeneratedCA">>, CA},
                 {<<"autogeneratedCert">>, NewNodeCert}] ++
                case cluster_compat_mode:is_cluster_70() of
                    true ->
                        [{<<"autogeneratedKey">>, NewNodeKey}];
                    false ->
                        [{<<"autogeneratedKey">>, {[{otpCookie, NewNodeKey}]}}]
                end;
            true ->
                [{<<"autogeneratedCA">>, ns_server_cert:self_generated_ca()}]
        end ++ Props,

    Options = [{connect_options, [misc:get_net_family()]}],

    AllowedScheme = case proplists:is_defined(<<"autogeneratedKey">>, Props1) of
                         true -> Scheme == https;
                         false -> true
                    end,

    case AllowedScheme of
        true ->
            case post_json([Scheme, RemoteAddr, RestPort, "/engageCluster2"],
                           Auth, Options, {Props1}) of
                {ok, {struct, NodeKVList}} ->
                    try
                        do_add_node_engaged(NodeKVList, Auth, GroupUUID,
                                            Services, Scheme)
                    catch
                        exit:{unexpected_json, _Where, _Field} = Exc ->
                            engage_cluster_bad_json_error(Exc)
                    end;
                {ok, _JSON} ->
                    engage_cluster_bad_json_error(undefined);
                {error, invalid_json} ->
                    engage_cluster_bad_json_error(undefined);
                {error, Msg} ->
                    M = iolist_to_binary([<<"Prepare join failed. ">>, Msg]),
                    {error, engage_cluster, M}
            end;
        false ->
            {error, not_allowed_scheme,
             <<"http is prohibited due to security reasons, please use https">>}
    end.

expect_json_property_base(PropName, KVList) ->
    case lists:keyfind(PropName, 1, KVList) of
        false ->
            erlang:exit({unexpected_json, missing_property, PropName});
        Tuple -> element(2, Tuple)
    end.

expect_json_property_binary(PropName, KVList) ->
    RV = expect_json_property_base(PropName, KVList),
    case is_binary(RV) of
        true -> RV;
        _ -> erlang:exit({unexpected_json, not_binary, PropName})
    end.

expect_json_property_list(PropName, KVList) ->
    binary_to_list(expect_json_property_binary(PropName, KVList)).

expect_json_property_atom(PropName, KVList) ->
    binary_to_atom(expect_json_property_binary(PropName, KVList), latin1).

expect_json_property_integer(PropName, KVList) ->
    RV = expect_json_property_base(PropName, KVList),
    expect_integer(PropName, RV).

expect_integer(PropName, Value) ->
    case is_integer(Value) of
        true -> Value;
        false -> erlang:exit({unexpected_json, not_integer, PropName})
    end.

get_port_from_empd(OtpNode, AFamily, Encryption) ->
    Res = case cb_epmd:get_port(OtpNode, AFamily, Encryption, 5000)  of
              {port, Port, _Version} -> {ok, Port};
              noport -> {error, noport};
              {error, _} = Error -> Error
          end,
    ?cluster_debug("port_please(~p, ~p, ~p) = ~p",
                   [OtpNode, AFamily, Encryption, Res]),
    Res.

verify_otp_connectivity(OtpNode, Options) ->
    NodeAFamily = proplists:get_value(node_afamily, Options,
                                      cb_dist:address_family()),
    NodeEncryption = proplists:get_value(node_encryption, Options,
                                         cb_dist:external_encryption()),
    TCPOnly = proplists:get_bool(tcp_only, Options),
    Host = misc:extract_node_address(OtpNode, NodeAFamily),
    PortRes =
        case proplists:get_value(port, Options) of
            undefined ->
                get_port_from_empd(OtpNode, NodeAFamily, NodeEncryption);
            P ->
                {ok, P}
        end,
    case PortRes of
        {ok, Port} ->
            VerifyConnectivity =
                case {NodeEncryption, TCPOnly} of
                    {true, false} -> fun check_otp_tls_connectivity/3;
                    _ -> fun check_host_port_connectivity/3
                end,
            case VerifyConnectivity(Host, Port, NodeAFamily) of
                {ok, IP} -> {ok, IP};
                {error, Reason} ->
                    {error, connect_node,
                     ns_error_messages:verify_otp_connectivity_connection_error(
                       Reason, OtpNode, Host, Port)}
            end;
        Error ->
            {error, connect_node,
             ns_error_messages:verify_otp_connectivity_port_error(OtpNode, Host,
                                                                  Error)}
    end.

check_otp_tls_connectivity(Host, Port, AFamily) ->
    %% Building connect options the same way inet_tls_dist builds it.
    %% Note that ssl_dist_opts ets is populated by erlang ssl app (in our case
    %% it should contain options from /etc/ssl_dist_opts.in).
    [{client, TLSOpts}] = ets:lookup(ssl_dist_opts, client),
    Opts = application:get_env(kernel, inet_dist_connect_options, []),
    %% cb_dist might not have updated the password in ssl_dist_opts yet,
    %% so update it here
    PassFun = ns_secrets:get_pkey_pass(),
    Opts2 = misc:update_proplist(Opts, [{password, PassFun()}]),
    SNIOpts = case inet:parse_address(Host) of
                  {ok, _} -> [];
                  _ -> [{server_name_indication, Host}]
              end,
    AllOpts = [binary, {active, false}, {packet, 4},
               AFamily, {nodelay, true}, {erl_dist, true}] ++ SNIOpts ++
               lists:ukeysort(1, Opts2 ++ TLSOpts),
    Timeout = net_kernel:connecttime(),
    case inet:getaddr(Host, AFamily) of
        {ok, IpAddr} ->
            case ssl:connect(IpAddr, Port, AllOpts, Timeout) of
                {ok, TLSSocket} ->
                    {ok, {LocalIpAddr, _}} = ssl:sockname(TLSSocket),
                    catch ssl:close(TLSSocket),
                    ?log_debug("Successfully checked OTP TLS connectivity to "
                               "~p(~p):~p", [Host, IpAddr, Port]),
                    {ok, inet:ntoa(LocalIpAddr)};
                {error, _} = Error ->
                    Error
            end;
        {error, _} = Error ->
            Error
    end.

do_add_node_engaged(NodeKVList, Auth, GroupUUID, Services, Scheme) ->
    OtpNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),

    VerifyOpts = case ns_server_cert:this_node_uses_self_generated_certs() of
                     %% Do not test TLS connection in case of self-generated
                     %% certs.
                     %% Reason: the remote node can't have correct certs
                     %% generated at this point.
                     %% The remote node will establish TLS dist connection
                     %% to us (because it knows our CA), then it will sync
                     %% config, and only then it will be able to generate
                     %% correct certs
                     %% Update: starting from 7.0 we pass certs to the node
                     %% being added, so it should work even in case of
                     %% self-generated certs.
                     true ->
                         case cluster_compat_mode:is_cluster_70() of
                             true -> [];
                             false -> [tcp_only]
                         end;
                     false -> []
                 end,
    RV = verify_otp_connectivity(OtpNode, VerifyOpts),
    case RV of
        {ok, _} ->
            case check_can_add_node(NodeKVList) of
                ok ->
                    RequestTarget = get_request_target(NodeKVList, Scheme),
                    %% TODO: only add services if possible
                    %% TODO: consider getting list of supported
                    %% services from NodeKVList
                    node_add_transaction(
                      OtpNode, GroupUUID, Services,
                      do_add_node_engaged_inner(
                        _, RequestTarget, OtpNode, Auth, Services));
                Error -> Error
            end;
        X -> X
    end.

check_can_add_node(NodeKVList) ->
    JoineeClusterCompatVersion = expect_json_property_integer(<<"clusterCompatibility">>, NodeKVList),
    JoineeNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),

    MyCompatVersion = cluster_compat_mode:effective_cluster_compat_version(),
    case JoineeClusterCompatVersion =:= MyCompatVersion of
        true -> case expect_json_property_binary(<<"version">>, NodeKVList) of
                    <<"1.",_/binary>> = Version ->
                        {error, incompatible_cluster_version,
                         ns_error_messages:too_old_version_error(JoineeNode,
                                                                 Version)};
                    _ ->
                        ok
                end;
        false ->
            {error, incompatible_cluster_version,
             ns_error_messages:incompatible_cluster_version_error(
               MyCompatVersion, JoineeClusterCompatVersion, JoineeNode)}
    end.

get_request_target(NodeKVList, Scheme) ->
    HostnameRaw = expect_json_property_list(<<"hostname">>, NodeKVList),
    {Hostname, NonHttpsPort} = misc:split_host_port(HostnameRaw, "8091",
                                                     misc:get_net_family()),
    {struct, Ports} = proplists:get_value(<<"ports">>, NodeKVList,
                                          {struct, []}),
    {Scheme2, Port} =
        case proplists:get_value(<<"httpsMgmt">>, Ports) of
            undefined -> {http, NonHttpsPort};
            P when Scheme == https -> {https, P};
            _ -> {http, NonHttpsPort}
        end,
    {Scheme2, Hostname, Port}.

do_add_node_engaged_inner(ChronicleInfo, {Scheme, Hostname, Port},
                          OtpNode, Auth, Services) ->
    {struct, MyNodeKVList} =
        menelaus_web_node:build_full_node_info(node()),
    Struct = {struct, [{<<"targetNode">>, OtpNode},
                       {<<"requestedServices">>, Services}
                       | MyNodeKVList] ++
                  [{<<"chronicleInfo">>,
                    base64:encode(term_to_binary(ChronicleInfo))} ||
                      ChronicleInfo =/= undefined]},

    Options = [{connect_options, [misc:get_net_family()]},
               {timeout, ?COMPLETE_TIMEOUT},
               {connect_timeout, ?COMPLETE_TIMEOUT}],

    case post_json([Scheme, Hostname, Port, "/completeJoin"],
                   Auth, Options, Struct) of
        {ok, _} ->
            {ok, OtpNode};
        {error, Msg} ->
            {error, complete_join,
            case Msg of
                invalid_json ->
                    <<"REST call returned invalid json.">>;
                _ ->
                    iolist_to_binary([<<"Join completion call failed. ">>, Msg])
            end}
    end.

node_add_transaction(Node, GroupUUID, Services, Body) ->
    case chronicle_master:add_replica(Node, GroupUUID, Services) of
        ok ->
            %% pre 7.0 clusters only
            node_add_transaction_finish(Node, GroupUUID, undefined, Body);
        {ok, ChronicleInfo} ->
            node_add_transaction_finish(Node, GroupUUID, ChronicleInfo, Body);
        cannot_acquire_lock ->
            {error, cannot_acquire_lock,
             <<"Operation temporarily cannot be performed possibly due to loss "
               "of quorum">>};
        group_not_found ->
            M = iolist_to_binary([<<"Could not find group with uuid: ">>,
                                  GroupUUID]),
            {error, unknown_group, M};
        node_present ->
            M = iolist_to_binary([<<"Node already exists in cluster: ">>,
                                  atom_to_list(Node)]),
            {error, node_present, M};
        unfinished_failover ->
            {error, unfinished_failover,
             <<"Operation cannot be performed due to unfinished failover">>}
    end.

node_add_transaction_finish(Node, GroupUUID, ChronicleInfo, Body) ->
    ?cluster_info("Started node add transaction by adding node ~p to nodes_wanted (group: ~s)",
                  [Node, GroupUUID]),
    try Body(ChronicleInfo) of
        {ok, _} = X -> X;
        Crap ->
            ?cluster_error("Add transaction of ~p failed because of ~p",
                           [Node, Crap]),
            shun(Node),
            Crap
    catch
        Type:What:Stack ->
            ?cluster_error("Add transaction of ~p failed because of exception ~p",
                           [Node, {Type, What, Stack}]),
            shun(Node),
            erlang:Type(What),
            erlang:error(cannot_happen)
    end.

do_engage_cluster(NodeKVList) ->
    try
        do_engage_cluster_check_compatibility(NodeKVList)
    catch
        exit:{unexpected_json, _, _} = Exc ->
            {error, unexpected_json,
             ns_error_messages:engage_cluster_json_error(Exc)}
    end.

do_engage_cluster_check_compatibility(NodeKVList) ->
    Version = expect_json_property_binary(<<"version">>, NodeKVList),
    Node = expect_json_property_atom(<<"otpNode">>, NodeKVList),

    case Version of
        <<"1.",_/binary>> ->
            {error, incompatible_cluster_version,
             ns_error_messages:too_old_version_error(Node, Version)};
        _ ->
            do_engage_cluster_check_compat_version(Node, Version, NodeKVList)
    end.

do_engage_cluster_check_compat_version(Node, Version, NodeKVList) ->
    ActualCompatibility = expect_json_property_integer(<<"clusterCompatibility">>, NodeKVList),
    MinSupportedCompatVersion = cluster_compat_mode:min_supported_compat_version(),
    MinSupportedCompatibility =
        cluster_compat_mode:effective_cluster_compat_version_for(MinSupportedCompatVersion),
    IsPreviewCluster =
        proplists:get_value(<<"isDeveloperPreview">>, NodeKVList, false),
    SupportedVsn = cluster_compat_mode:supported_compat_version(),
    WantedCompatibility =
        cluster_compat_mode:effective_cluster_compat_version_for(SupportedVsn),

    if
        ActualCompatibility < MinSupportedCompatibility ->
            {error, incompatible_cluster_version,
             ns_error_messages:too_old_version_error(Node, Version)};
        IsPreviewCluster and (WantedCompatibility > ActualCompatibility) ->
            {error, incompatible_cluster_version,
             ns_error_messages:preview_cluster_join_error()};
        true ->
            do_engage_cluster_check_services(NodeKVList)
    end.

get_requested_services(KVList) ->
    Default = ns_cluster_membership:default_services(),
    do_get_requested_services(<<"requestedServices">>, KVList, Default).

do_get_requested_services(Key, KVList, Default) ->
    case lists:keyfind(Key, 1, KVList) of
        false ->
            {ok, Default};
        {_, List} ->
            case is_list(List) of
                false ->
                    erlang:exit({unexpected_json, not_list, Key});
                _ ->
                    case [[] || B <- List, not is_binary(B)] =:= [] of
                        false ->
                            erlang:exit({unexpected_json, not_list, Key});
                        true ->
                            try
                                {ok, [binary_to_existing_atom(S, latin1) || S <- List]}
                            catch
                                error:badarg ->
                                    {bad_services, List}
                            end
                    end
            end
    end.

community_allowed_topologies() ->
    KvOnly = [kv],
    AllServices40 = [kv, index, n1ql],
    AllServices = ns_cluster_membership:supported_services(false),

    [KvOnly, lists:sort(AllServices40), lists:sort(AllServices)].

enforce_topology_limitation(Services) ->
    case cluster_compat_mode:is_enterprise() of
        true ->
            ok;
        false ->
            SortedServices = lists:sort(Services),
            SupportedCombinations = community_allowed_topologies(),
            case lists:member(SortedServices, SupportedCombinations) of
                true ->
                    ok;
                false ->
                    {error, ns_error_messages:topology_limitation_error(SupportedCombinations)}
            end
    end.

do_engage_cluster_check_services(NodeKVList) ->
    SupportedServices = ns_cluster_membership:supported_services(),
    case get_requested_services(NodeKVList) of
        {ok, Services} ->
            case Services -- SupportedServices of
                [] ->
                    case enforce_topology_limitation(Services) of
                        {error, Msg} ->
                            {error, incompatible_services, Msg};
                        ok ->
                            do_engage_cluster_inner(NodeKVList, Services)
                    end;
                _ ->
                    unsupported_services_error(SupportedServices, Services)
            end;
        {bad_services, RawServices} ->
            unsupported_services_error(SupportedServices, RawServices)
    end.

unsupported_services_error(Supported, Requested) ->
    {error, incompatible_services,
     ns_error_messages:unsupported_services_error(Supported, Requested)}.

do_engage_cluster_inner(NodeKVList, Services) ->
    OtpNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),
    MaybeTargetHost = proplists:get_value(<<"requestedTargetNodeHostname">>, NodeKVList),
    case verify_otp_connectivity(OtpNode, []) of
        {ok, MyIP} ->
            {Address, UserSupplied} =
                case MaybeTargetHost of
                    undefined ->
                        {MyIP, false};
                    _ ->
                        TargetHost = binary_to_list(MaybeTargetHost),
                        {TargetHost, true}
                end,

            case do_engage_cluster_inner_check_address(Address, UserSupplied) of
                ok ->
                    do_engage_cluster_inner_tail(NodeKVList, Address, UserSupplied, Services);
                Error ->
                    Error
            end;
        X -> X
    end.

do_engage_cluster_inner_check_address(_Address, false) ->
    ok;
do_engage_cluster_inner_check_address(Address, true) ->
    case misc:is_good_address(Address) of
        ok ->
            ok;
        {ErrorType, _} = Error ->
            Msg = ns_error_messages:address_check_error(Address, Error),
            {error, ErrorType, Msg}
    end.

do_engage_cluster_inner_tail(NodeKVList, Address, UserSupplied, Services) ->
    case do_change_address(Address, UserSupplied) of
        {address_save_failed, Error1} ->
            Msg = io_lib:format("Could not save address after rename: ~p",
                                [Error1]),
            {error, rename_failed, iolist_to_binary(Msg)};
        _ ->
            %% we re-init node's cookie to support joining cloned
            %% nodes. If we don't do that cluster will be able to
            %% connect to this node too soon. And then initial set of
            %% nodes_wanted by node thats added to cluster may
            %% 'pollute' cluster's version and cause issues. See
            %% MB-4476 for details.
            ns_cookie_manager:cookie_init(),
            check_can_join_to(NodeKVList, Services)
    end.

json_field(Service) ->
    list_to_binary(atom_to_list(memory_quota:service_to_json_name(Service))).

check_memory_size(NodeKVList, Services) ->
    Quotas =
        lists:foldl(
          fun(kv, Acc) ->
                  Quota = expect_json_property_integer(json_field(kv), NodeKVList),
                  [{kv, Quota} | Acc];
             (Service, Acc) ->
                  JSONField = json_field(Service),
                  case lists:keyfind(JSONField, 1, NodeKVList) of
                      {_, Quota} ->
                          [{Service, expect_integer(JSONField, Quota)} | Acc];
                      false ->
                          Acc
                  end
          end, [], memory_quota:aware_services()),

    case memory_quota:check_this_node_quotas(Services, Quotas) of
        ok ->
            ok;
        {error, {total_quota_too_high, _, TotalQuota, MaxQuota}} ->
            {error, bad_memory_size,
             ns_error_messages:bad_memory_size_error(Services, TotalQuota,
                                                     MaxQuota)}
    end.

check_can_join_to(NodeKVList, Services) ->
    case check_memory_size(NodeKVList, Services) of
        ok -> {ok, ok};
        Error -> Error
    end.

get_chronicle_info(KVList) ->
    case proplists:get_value(<<"chronicleInfo">>, KVList) of
        undefined ->
            undefined;
        Binary ->
            binary_to_term(base64:decode(Binary))
    end.

-spec do_complete_join([{binary(), term()}]) -> {ok, ok} | {error, atom(),
                                                            binary()}.
do_complete_join(NodeKVList) ->
    try
        OtpNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),
        OtpCookie = expect_json_property_atom(<<"otpCookie">>, NodeKVList),
        MyNode = expect_json_property_atom(<<"targetNode">>, NodeKVList),

        {ok, Services} = get_requested_services(NodeKVList),
        case check_can_join_to(NodeKVList, Services) of
            {ok, _} ->
                case ns_cluster_membership:system_joinable() andalso MyNode =:= node() of
                    false ->
                        {error, join_race_detected,
                         <<"Node is already part of cluster.">>};
                    true ->
                        perform_actual_join(
                          OtpNode, OtpCookie, get_chronicle_info(NodeKVList))
                end;
            Error -> Error
        end
    catch exit:{unexpected_json, _Where, _Field} ->
            {error, engage_cluster_bad_json,
             ns_error_messages:engage_cluster_json_error(undefined)}
    end.


perform_actual_join(RemoteNode, NewCookie, ChronicleInfo) ->
    ?cluster_log(0002, "Node ~p is joining cluster via node ~p.",
                 [node(), RemoteNode]),
    %% let ns_memcached know that we don't need to preserve data at all
    ns_config:set(i_am_a_dead_man, true),

    %% empty users storage
    menelaus_users:empty_storage(),

    %% Pull the rug out from under the app
    misc:create_marker(start_marker_path()),
    ok = ns_server_cluster_sup:stop_ns_server(),
    ns_log:delete_log(),

    ?cluster_debug("ns_cluster: joining cluster. Child has exited.", []),
    misc:create_marker(join_marker_path()),
    ns_cluster_membership:prepare_to_join(RemoteNode, NewCookie),

    ok = chronicle_local:prepare_join(ChronicleInfo),

    %% reload is needed to reinitialize ns_config's cache after
    %% config cleanup ('erase' causes the problem, but it looks like
    %% it's not worth it to add proper 'erase' support to ns_config)
    ns_config:reload(),
    ns_config:merge_dynamic_and_static(),
    ?cluster_debug("pre-join cleaned config is:~n~p",
                   [ns_config_log:sanitize(ns_config:get())]),

    {ok, _Cookie} = ns_cookie_manager:cookie_sync(),
    %% Let's verify connectivity.

    Connected =
        misc:poll_for_condition(
          fun () ->
                  ?log_debug("Trying to connect to node ~p...", [RemoteNode]),
                  case catch net_kernel:connect_node(RemoteNode) of
                      true -> true;
                      Res ->
                          ?log_error("Connect to node ~p failed: ~p",
                                     [RemoteNode, Res]),
                          false
                  end
          end, 10000, 500),

    ?cluster_debug("Connection from ~p to ~p:  ~p",
                   [node(), RemoteNode, Connected]),

    ok = chronicle_local:join_cluster(ChronicleInfo),

    %% Make sure that latest timestamps are published synchronously.
    tombstone_agent:refresh(),

    ok = ns_config_rep:pull_from_one_node_directly(RemoteNode),
    ?cluster_debug("pre-join merged config is:~n~p",
                   [ns_config_log:sanitize(ns_config:get())]),

    misc:remove_marker(join_marker_path()),

    ?cluster_debug("Join succeded, starting ns_server_cluster back", []),
    case ns_server_cluster_sup:start_ns_server() of
        {error, _} = Error ->
            ?cluster_error("Failed to join cluster because of: ~p", [Error]),
            {error, start_cluster_failed,
             <<"Failed to start ns_server cluster processes back. "
               "Logs might have more details.">>};
        {ok, _} ->
            misc:remove_marker(start_marker_path()),
            ?cluster_log(?NODE_JOINED, "Node ~s joined cluster", [node()]),
            {ok, ok}
    end.

perform_leave() ->
    chronicle_local:leave_cluster(),

    prometheus_cfg:wipe(),

    %% in order to disconnect from rest of nodes we need new cookie
    %% and explicit disconnect_node calls
    {ok, _} = ns_cookie_manager:cookie_init(),

    %% reset_address() below drops user_assigned flag (if any) which makes
    %% it possible for the node to be renamed if necessary
    ok = dist_manager:reset_address(),
    %% and then we clear config. In fact better name would be 'reset',
    %% because as seen above we actually re-initialize default config
    tombstone_agent:wipe(),

    ns_config:clear([directory,
                     %% Preserve these directories as they may have been
                     %% changed from their defaults and their handling
                     %% should be consistent with the way we retain the
                     %% index and data directories.
                     {node, node(), cbas_dirs},
                     {node, node(), eventing_dir},
                     %% we preserve rest settings, so if the server runs on a
                     %% custom port, it doesn't revert to the default
                     rest,
                     {node, node(), rest},
                     {node, node(), address_family},
                     {node, node(), address_family_only},
                     {node, node(), node_encryption},
                     {node, node(), erl_external_listeners}]),


    %% set_initial here clears vclock on nodes_wanted. Thus making
    %% sure that whatever nodes_wanted we will get through initial
    %% config replication (right after joining cluster next time) will
    %% not conflict with this value.
    ns_config:set_initial(nodes_wanted, [node()]),
    {ok, _} = ns_cookie_manager:cookie_sync().

leave_marker_path() ->
    path_config:component_path(data, "leave_marker").

start_marker_path() ->
    path_config:component_path(data, "start_marker").

rename_marker_path() ->
    path_config:component_path(data, "rename_marker").

join_marker_path() ->
    path_config:component_path(data, "join_marker").

-ifdef(TEST).
community_allowed_topologies_test() ->
    %% Test to help catch changes in community topologies that don't
    %% maintain backwards compatibility
    ?assertEqual(community_allowed_topologies(),
                 [[kv],[index,kv,n1ql],[fts,index,kv,n1ql]]).

-endif.
